{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User guide of the pipeline executors usage\n",
    "\n",
    "Bosk pipelines can be fitted and executed with the different managers called *executors*. In this user guide we will explain the semantics of the executor interface and show, how you can customize the executor's behaviour. Particular class that we will use is `RecursiveExecutor`. `RecursiveExecutor` is a simple executor that for each output recursively computes data and thus traverses a computational graph backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bosk.executor.recursive import RecursiveExecutor\n",
    "from bosk.executor.timer import TimerBlockExecutor\n",
    "from bosk.stages import Stage\n",
    "from bosk.pipeline.builder.functional import FunctionalPipelineBuilder\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple deep forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 15\n",
    "b = FunctionalPipelineBuilder()\n",
    "# that we do as it was described above\n",
    "X, y = b.Input()(), b.TargetInput()()\n",
    "rf_1 = b.RFC(n_estimators=n_estimators)(X=X, y=y)\n",
    "et_1 = b.ETC(n_estimators=n_estimators)(X=X, y=y)\n",
    "concat_1 = b.Concat(['X', 'rf_1', 'et_1'])(X=X, rf_1=rf_1, et_1=et_1)\n",
    "rf_2 = b.RFC(n_estimators=n_estimators)(X=concat_1, y=y)\n",
    "et_2 = b.ETC(n_estimators=n_estimators)(X=concat_1, y=y)\n",
    "stack = b.Stack(['rf_2', 'et_2'], axis=1)(rf_2=rf_2, et_2=et_2)\n",
    "average = b.Average(axis=1)(X=stack)\n",
    "argmax = b.Argmax(axis=1)(X=average)\n",
    "rf_1_roc_auc = b.RocAuc()(gt_y=y, pred_probas=rf_1)\n",
    "roc_auc = b.RocAuc()(gt_y=y, pred_probas=average)\n",
    "pipeline = b.build(\n",
    "    {'X': X, 'y': y},\n",
    "    {'labels': argmax, 'probas': average, 'rf_1_roc-auc': rf_1_roc_auc, 'roc-auc': roc_auc}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this pipeline with `RecursiveExecutor`. As it was said in \"Example of the basic bosk usage\", for each stage we have to create a new executor instance. The most simple way to create executor is to pass your pipeline and the computational stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_exec = RecursiveExecutor(pipeline, Stage.FIT)\n",
    "# let's make some data to train our model\n",
    "X, y = make_moons(noise=0.5)\n",
    "# now we need to create a dictionary to map the data to the\n",
    "# pipeline's inputs\n",
    "full_data = {'X': X, 'y': y}\n",
    "# to run the executor we need to pass the data\n",
    "fit_res = fit_exec(full_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to understand that to fit a layer we need to pass a data through the previous one. So during the fit stage every block is fitted, than the transform method is called. Therefore on the fit stage we have transformation of the training data as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'probas', 'rf_1_roc-auc', 'roc-auc']\n"
     ]
    }
   ],
   "source": [
    "print(list(fit_res.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another executor for the transform stage, but now we will pay the attention to the `inputs` and `outputs` arguments. They set a constraint on the corresponding pipeline attributes. Passing `inputs`, you set up a hard requirement for the input values to execute the computational graph. It means that if you specify `inputs`, input data must contain only those keys. `outputs` specify graph's outputs that will be proceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input \"y\" is not in the executor's inputs set\n"
     ]
    }
   ],
   "source": [
    "# let's specify inputs and outputs\n",
    "tf_exec = RecursiveExecutor(pipeline, Stage.TRANSFORM, ['X'], ['probas'])\n",
    "# let's try to break out inputs requirement\n",
    "try:\n",
    "    tf_exec(full_data)\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is needed to say that if we didn't specify the inputs, the exception wouldn't be raised even if we had some keys except `X` and `y` in the input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['probas']\n"
     ]
    }
   ],
   "source": [
    "# now let's make the right dictionary\n",
    "tf_data = {'X': X}\n",
    "tf_res = tf_exec(tf_data)\n",
    "print(list(tf_res.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to discuss `slot_handler` and `block_executor` arguments. Those ones are needed to perform user customization of the executor behaviour. As the names imply, `slot_handler` is responsible for the slots handling policy and `block_executor` - blocks. User can implement his own inheritors of `BaseSlotHandler` and `BaseBlockExecutor` respectively and thus implement some additional logic during the pipeline execution. In the examle below we will measure blocks execution time using custom `block_executor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most complex block is RFCBlock (id 8759314183514). Execution time is 0.00369 s.\n"
     ]
    }
   ],
   "source": [
    "timer_block_exec = TimerBlockExecutor()\n",
    "tf_exec = RecursiveExecutor(pipeline, Stage.TRANSFORM, \n",
    "                            outputs=['labels'],\n",
    "                            block_executor=timer_block_exec)\n",
    "tf_exec(tf_data)\n",
    "# this block executor stores cpu execution time\n",
    "# for each executed block\n",
    "# let's find the most complex block\n",
    "max_time_block = None\n",
    "max_time = 0\n",
    "for block, time in timer_block_exec.blocks_time.items():\n",
    "    if time > max_time:\n",
    "        max_time = time\n",
    "        max_time_block = block\n",
    "print(f'The most complex block is {max_time_block} (id { hash(max_time_block)}).',\n",
    "      f'Execution time is {round(max_time, 5)} s.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
